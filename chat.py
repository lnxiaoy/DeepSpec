import chromadb
import ollama
from chromadb.utils import embedding_functions
from colorama import init, Fore

init(autoreset=True)

DB_PATH = "./ran1_knowledge_base"
MODEL_NAME = "qwen2.5:14b" 

def chat_loop():
    print(f"{Fore.CYAN}=== DeepSpec å…¨æ ˆä¸“å®¶ç³»ç»Ÿ (Spec + TDoc) ===")
    
    client = chromadb.PersistentClient(path=DB_PATH)
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")
    
    # è·å–ä¸¤ä¸ªé›†åˆ
    try:
        coll_specs = client.get_collection(name="ran1_specs", embedding_function=ef)
        coll_tdocs = client.get_collection(name="ran1_docs", embedding_function=ef)
    except:
        print(f"{Fore.RED}é”™è¯¯ï¼šè¯·ç¡®ä¿ä½ å·²ç»åˆ†åˆ«è¿è¡Œäº† indexer.py (TDoc) å’Œ indexer_specs.py (Spec)ï¼")
        return

    while True:
        query = input(f"\n{Fore.YELLOW}è¯·æé—® (e.g. 38.211é‡ŒDMRSæ€ä¹ˆå®šä¹‰çš„? å„å®¶æƒ³æ€ä¹ˆæ”¹?): {Fore.RESET}")
        if query.lower() in ["exit", "quit"]: break
        
        print(f"{Fore.CYAN}ğŸ” 1. æ­£åœ¨æŸ¥é˜… 3GPP æ³•å¾‹æ¡æ–‡ (Specs)...")
        res_specs = coll_specs.query(query_texts=[query], n_results=3)
        
        print(f"{Fore.CYAN}ğŸ” 2. æ­£åœ¨æŸ¥é˜… å‚å•†ææ¡ˆ (TDocs)...")
        res_tdocs = coll_tdocs.query(query_texts=[query], n_results=5)
        
        # ç»„è£…ä¸Šä¸‹æ–‡
        context_str = "ã€Part 1: ç°æœ‰æ ‡å‡†å®šä¹‰ (Ground Truth)ã€‘\n"
        for doc in res_specs['documents'][0]:
            context_str += f"{doc}\n---\n"
            
        context_str += "\nã€Part 2: æœ¬æ¬¡ä¼šè®®çš„ææ¡ˆä¸äº‰è®® (Debate)ã€‘\n"
        for i, doc in enumerate(res_tdocs['documents'][0]):
            fname = res_tdocs['metadatas'][0][i]['filename']
            context_str += f"Source: {fname}\nContent: {doc}\n---\n"

        # è®©æ¨¡å‹ç»¼åˆ
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ 3GPP æ ‡å‡†æ¶æ„å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”é—®é¢˜ã€‚
        
        ã€èµ„æ–™ç»“æ„ã€‘ï¼š
        1. **ç°æœ‰æ ‡å‡†**ï¼šæ¥è‡ª 38.211/38.213 ç­‰ Specï¼Œè¿™æ˜¯å½“å‰çš„æ³•å¾‹åŸºå‡†ã€‚
        2. **ä¼šè®®ææ¡ˆ**ï¼šæ¥è‡ªå„å‚å•†çš„ TDocï¼Œè¿™æ˜¯ä»–ä»¬æƒ³ä¿®æ”¹æˆ–å¢å¼ºçš„åœ°æ–¹ã€‚
        
        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
        {query}
        
        ã€å›ç­”é€»è¾‘ã€‘ï¼š
        1. å…ˆå¼•ç”¨ Specï¼Œç®€è¿°**å½“å‰æ ‡å‡†**æ˜¯å¦‚ä½•è§„å®šçš„ï¼ˆå¼•ç”¨ç« èŠ‚å·ï¼‰ã€‚
        2. å†å¼•ç”¨ TDocï¼Œé˜è¿°**å„å‚å•†**æå‡ºäº†ä»€ä¹ˆæ–°è§‚ç‚¹æˆ–ä¿®æ”¹å»ºè®®ã€‚
        3. ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸“ä¸šã€å‡†ç¡®ã€‚
        
        ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
        {context_str}
        """
        
        print(f"{Fore.GREEN}ğŸ¤– Qwen æ­£åœ¨æ€è€ƒ...")
        stream = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': prompt}], stream=True)
        
        print(f"{Fore.WHITE}", end="")
        for chunk in stream:
            print(chunk['message']['content'], end="", flush=True)
        print("\n")

if __name__ == "__main__":
    chat_loop()
